{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, Conversation, LlamaForCausalLM, LlamaTokenizer, LlamaTokenizerFast\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import json\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model and tokenizer add global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'local_model/tiny_llama'\n",
    "TOKENIZER_PATH = 'local_tokenizer/tiny_llama'\n",
    "MAX_ANSWER_LEN = 128\n",
    "MIN_ANSWER_LEN = 12\n",
    "RETURN_SENTENCE = 1\n",
    "BATCH_SIZE = 1\n",
    "torch.manual_seed(21)\n",
    "TOKENIZER = LlamaTokenizerFast.from_pretrained(TOKENIZER_PATH, model_max_length=MAX_ANSWER_LEN)\n",
    "MODEL = LlamaForCausalLM.from_pretrained(MODEL_PATH,\n",
    "                                        device_map='auto',\n",
    "                                        torch_dtype=torch.float16,\n",
    "                                        low_cpu_mem_usage=True,\n",
    "                                        max_position_embeddings=MAX_ANSWER_LEN\n",
    "                                        )\n",
    "DEVICE =  'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_EOS = TOKENIZER.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='local_tokenizer/tiny_llama', vocab_size=32000, model_max_length=128, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad06f7bcbf142d6a719a4ca923c2343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# youtube guild "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_bot = pipeline(task='text-generation',\n",
    "                    model=MODEL,\n",
    "                    tokenizer=TOKENIZER,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    max_new_token_length=MAX_ANSWER_LEN,\n",
    "                    do_sample = True,\n",
    "                    top_k=5,\n",
    "                    num_return_sentences=RETURN_SENTENCE,\n",
    "                    eos_token_id=TOKEN_EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST = '[INST]'\n",
    "E_INST = '[/INST]'\n",
    "B_SYS = '<<SYS>>\\n'\n",
    "E_SYS = '\\n<</SYS>>\\n\\n'\n",
    "DEFAULT_PROMPT = '''\\\\\n",
    "Answer user complains as tech support worker \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruct:str, new_sys_prompt:str=DEFAULT_PROMPT) -> str:\n",
    "    SYS_PROMPT = B_SYS + new_sys_prompt + E_SYS\n",
    "    template = B_INST + SYS_PROMPT + instruct + B_INST\n",
    "    return template\n",
    "\n",
    "def cut_off_text(text:str, prompt:str) -> str:\n",
    "    idx = text.find(prompt)\n",
    "    if idx:\n",
    "        text = text[:idx]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_sub(string:str, sub:str) -> str:\n",
    "    return string.sub(sub, '')\n",
    "\n",
    "def generator(text:str) -> str:\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast(DEVICE, dtype=torch.float16):\n",
    "        inputs = TOKENIZER(prompt, return_tensors='pt').to(DEVICE)\n",
    "        dirty_output = MODEL.generate(**inputs,\n",
    "                                      max_new_tokens=MAX_ANSWER_LEN,\n",
    "                                      eos_token_id=TOKENIZER.eos_token_id,\n",
    "                                      pad_token_id=TOKENIZER.pad_token_id)\n",
    "        output = TOKENIZER.batch_decode(dirty_output, skip_special_tokens=True)[0]\n",
    "        output = cut_off_text(output, '</s>')\n",
    "        output = remove_sub(output, prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = 'Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}'\n",
    "llm = HuggingFacePipeline(pipeline=chat_bot, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = DEFAULT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = get_prompt(instruct, sys_prompt)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['chat_history', 'user_input'],\n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.predict(user_input=\"The estimated delivery time keeps changing! Now it says it'll be another hour?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
