{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pass.json') as pas:\n",
    "    hugging_face_token = json.load(pas)['hugging_face_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\jrtit\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, notebook_login\n",
    "login(hugging_face_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline,  LlamaForCausalLM, LlamaTokenizerFast\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model and tokenizer add global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'local_model/tiny_llama'\n",
    "TOKENIZER_PATH = 'local_tokenizer/tiny_llama'\n",
    "MAX_ANSWER_LEN = 128\n",
    "MIN_ANSWER_LEN = 12\n",
    "RETURN_SENTENCE = 1\n",
    "BATCH_SIZE = 1\n",
    "torch.manual_seed(21)\n",
    "TOKENIZER = LlamaTokenizerFast.from_pretrained(TOKENIZER_PATH, model_max_length=MAX_ANSWER_LEN)\n",
    "MODEL = LlamaForCausalLM.from_pretrained(MODEL_PATH,\n",
    "                                        device_map='auto',\n",
    "                                        torch_dtype=torch.half,\n",
    "                                        low_cpu_mem_usage=True,\n",
    "                                        max_position_embeddings=MAX_ANSWER_LEN,\n",
    "                                        \n",
    "                                        )\n",
    "DEVICE =  'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_EOS = TOKENIZER.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='local_tokenizer/tiny_llama', vocab_size=32000, model_max_length=128, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube guild  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_bot = pipeline(task='text-generation',\n",
    "                    model=MODEL, # type: ignore\n",
    "                    tokenizer=TOKENIZER,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    #max_new_token_length=MAX_ANSWER_LEN,\n",
    "                    do_sample = True,\n",
    "                    top_k=5,\n",
    "                    #num_return_sentences=RETURN_SENTENCE,\n",
    "                    eos_token_id=TOKEN_EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST = '[INST]'\n",
    "E_INST = '[/INST]'\n",
    "B_SYS = '<<SYS>>\\n'\n",
    "E_SYS = '\\n<</SYS>>\\n\\n'\n",
    "DEFAULT_PROMPT = '''\\\\\n",
    "Answer user complains as tech support worker \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruct:str, new_sys_prompt:str=DEFAULT_PROMPT) -> str:\n",
    "    SYS_PROMPT = B_SYS + new_sys_prompt + E_SYS\n",
    "    return B_INST + SYS_PROMPT + instruct + B_INST\n",
    "\n",
    "def cut_off_text(text:str, prompt:str) -> str:\n",
    "    if idx := text.find(prompt):\n",
    "        text = text[:idx]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_sub(string:str, subs:str) -> str:\n",
    "    return string.replace(subs, '')\n",
    "\n",
    "def generator(text:str) -> str:\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast(DEVICE, dtype=torch.float16):\n",
    "        inputs = TOKENIZER(prompt, return_tensors='pt').to(DEVICE)\n",
    "        dirty_output = MODEL.generate(**inputs, # type: ignore\n",
    "                                        max_new_tokens=MAX_ANSWER_LEN,\n",
    "                                        eos_token_id=TOKENIZER.eos_token_id,\n",
    "                                        pad_token_id=TOKENIZER.pad_token_id)\n",
    "        output = TOKENIZER.batch_decode(dirty_output, skip_special_tokens=True)[0]\n",
    "        output = cut_off_text(output, '</s>')\n",
    "        output = remove_sub(output, prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = 'Chat History:\\n{chat_history} \\nUser: {user_input}'\n",
    "llm = HuggingFacePipeline(pipeline=chat_bot, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = DEFAULT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "\\\n",
      "Answer user complains as tech support worker \n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "{chat_history} \n",
      "\n",
      "User: {user_input}[INST]\n"
     ]
    }
   ],
   "source": [
    "template = get_prompt(instruct, sys_prompt)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['chat_history', 'user_input'],\n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "\\\n",
      "Answer user complains as tech support worker \n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      " \n",
      "\n",
      "User: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[INST]<<SYS>>\\n\\\\\\nAnswer user complains as tech support worker \\n\\n<</SYS>>\\n\\nChat History:\\n\\n \\n\\nUser: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]<<SYS>>\\n\\nChatbot: I'm sorry to disappoint you, I can't control the delivery timings for every individual customer. However, our delivery team is constantly monitoring the delivery schedule, and they will try to keep the estimated delivery time consistent with the current delivery schedules. If any issue or delay ar\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"The estimated delivery time keeps changing! Now it says it'll be another hour?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sting = \"[INST]<<SYS>>\\n\\\\\\nAnswer user complains as tech support worker \\n\\n<</SYS>>\\n\\nChat History:\\n\\n \\n\\nUser: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]<<SYS>>\\n\\nChatbot: I'm sorry to disappoint you, I can't control the delivery timings for every individual customer. However, our delivery team is constantly monitoring the delivery schedule, and they will try to keep the estimated delivery time consistent with the current delivery schedules. If any issue or delay ar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST]<<SYS>>\\n\\\\\\nAnswer user complains as tech support worker \\n\\n<</SYS>>\\n\\nChat History:\\n\\n \\n\\nUser: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]<<SYS>>\\n\\nChatbot: I'm sorry to disappoint you, I can't control the delivery timings for every individual customer. However, our delivery team is constantly monitoring the delivery schedule, and they will try to keep the estimated delivery time consistent with the current delivery schedules. If any issue or delay ar\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
