{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9805e28640e94629b275675ebb7b0f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jrtit\\.cache\\huggingface\\hub\\models--PY007--TinyLlama-1.1B-step-50K-105b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1c685dbed543d09a06863ab64f69d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "d:\\anaconda\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs üöÄüöÄ. The training has started on 2023-09-01.\n",
      "The goal is: create a small network that learns the language part and predicts its outputs in natural languages with an accuracy >5\\% at best for every task (easily trainable without any additional hardware cost). We use BARTSAttentions from Google Cloud Speech Recognition as our base model architecture because it already achieves competitive results by fine tuning them during development phases before release time! This means you are free to experiment or try things out yourself. For more information about what'll be done next please visit https://www.linkedin.com/posts/-miguelbartesattionesgke_projectdavidepra?trk=top-tier-projects&amp;sourceid =8c7fcfaea6#Tip%3D0xA<KEY> <reponame>zhaojieyuanjia/wuxianweixun.github\n",
      "---\n",
      "layout : single \n",
      "title: \"<NAME>\"\n",
      "author: ZHQJIYUAN JIAZHAO &lt;EMAIL>&gt;\n",
      "categories: bloggers\n",
      "tags: [blogger]   # put keywords here if necessary.\n",
      "image: /assets/images/uploadfile-img/china.png     // Set image used below.\n",
      "comments: true     \t       \t            /* Add comments --> */\n",
      "modified: '2020-10-13   ‰∏ãÂçà7:26:'           /// Modification date,\n",
      "header-includes:{include html}\n",
      "excerpt:# wuzhang xiaoyao zhiqiang\n",
      "lastfmuser:\"Wujushiyou\"\n",
      "---                         <!-- Include file name and description underneath each tag section-->\n",
      "\n",
      "## About me\n",
      "\n",
      "Hi thereÔºå I am WXFanChen. You may contact me\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch\n",
    "model = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs üöÄüöÄ. The training has started on 2023-09-01.',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=500,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
