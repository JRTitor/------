{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pass.json') as pas:\n",
    "    hugging_face_token = json.load(pas)['hugging_face_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\jrtit\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, notebook_login\n",
    "login(hugging_face_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline,  LlamaForCausalLM, LlamaTokenizerFast\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model and tokenizer add global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'local_model/tiny_llama'\n",
    "TOKENIZER_PATH = 'local_tokenizer/tiny_llama'\n",
    "MAX_ANSWER_LEN = 256\n",
    "MIN_ANSWER_LEN = 12\n",
    "RETURN_SENTENCE = 1\n",
    "BATCH_SIZE = 1\n",
    "torch.manual_seed(21)\n",
    "TOKENIZER = LlamaTokenizerFast.from_pretrained(TOKENIZER_PATH, model_max_length=MAX_ANSWER_LEN)\n",
    "MODEL = LlamaForCausalLM.from_pretrained(MODEL_PATH,\n",
    "                                        device_map='auto',\n",
    "                                        torch_dtype=torch.half,\n",
    "                                        low_cpu_mem_usage=True,\n",
    "                                        max_position_embeddings=MAX_ANSWER_LEN,\n",
    "                                        \n",
    "                                        )\n",
    "DEVICE =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TOKEN_EOS = TOKENIZER.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='local_tokenizer/tiny_llama', vocab_size=32000, model_max_length=128, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube guild  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_bot = pipeline(task='text-generation',\n",
    "                    model=MODEL, # type: ignore\n",
    "                    tokenizer=TOKENIZER,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    #max_new_token_length=MAX_ANSWER_LEN,\n",
    "                    do_sample = True,\n",
    "                    top_k=5,\n",
    "                    #num_return_sentences=RETURN_SENTENCE,\n",
    "                    eos_token_id=TOKEN_EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST = '[INST]'\n",
    "E_INST = '[/INST]'\n",
    "B_SYS = '<<SYS>>\\n'\n",
    "E_SYS = '\\n<</SYS>>\\n\\n'\n",
    "DEFAULT_PROMPT = '''\\\\\n",
    "Answer user complains as tech support worker \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruct:str, new_sys_prompt:str=DEFAULT_PROMPT) -> str:\n",
    "    SYS_PROMPT = B_SYS + new_sys_prompt + E_SYS\n",
    "    return B_INST + SYS_PROMPT + instruct + B_INST\n",
    "\n",
    "def cut_off_text(text:str, prompt:str) -> str:\n",
    "    if idx := text.find(prompt):\n",
    "        text = text[:idx]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_sub(string:str, subs:str) -> str:\n",
    "    return string.replace(subs, '')\n",
    "\n",
    "def generator(text:str) -> str:\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast(DEVICE, dtype=torch.float16):\n",
    "        inputs = TOKENIZER(prompt, return_tensors='pt').to(DEVICE)\n",
    "        dirty_output = MODEL.generate(**inputs, # type: ignore\n",
    "                                        max_new_tokens=MAX_ANSWER_LEN,\n",
    "                                        eos_token_id=TOKENIZER.eos_token_id,\n",
    "                                        pad_token_id=TOKENIZER.pad_token_id)\n",
    "        output = TOKENIZER.batch_decode(dirty_output, skip_special_tokens=True)[0]\n",
    "        output = cut_off_text(output, '</s>')\n",
    "        output = remove_sub(output, prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = 'Chat History:\\n{chat_history} \\nUser: {user_input}'\n",
    "llm = HuggingFacePipeline(pipeline=chat_bot, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = DEFAULT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "\\\n",
      "Answer user complains as tech support worker \n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "{chat_history} \n",
      "\n",
      "User: {user_input}[INST]\n"
     ]
    }
   ],
   "source": [
    "template = get_prompt(instruct, sys_prompt)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['chat_history', 'user_input'],\n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "\\\n",
      "Answer user complains as tech support worker \n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      " \n",
      "\n",
      "User: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[INST]<<SYS>>\\n\\\\\\nAnswer user complains as tech support worker \\n\\n<</SYS>>\\n\\nChat History:\\n\\n \\n\\nUser: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]<<SYS>>\\n\\nChatbot: I'm sorry to disappoint you, I can't control the delivery timings for every individual customer. However, our delivery team is constantly monitoring the delivery schedule, and they will try to keep the estimated delivery time consistent with the current delivery schedules. If any issue or delay ar\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"The estimated delivery time keeps changing! Now it says it'll be another hour?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sting = \"[INST]<<SYS>>\\n\\\\\\nAnswer user complains as tech support worker \\n\\n<</SYS>>\\n\\nChat History:\\n\\n \\n\\nUser: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]<<SYS>>\\n\\nChatbot: I'm sorry to disappoint you, I can't control the delivery timings for every individual customer. However, our delivery team is constantly monitoring the delivery schedule, and they will try to keep the estimated delivery time consistent with the current delivery schedules. If any issue or delay ar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST]<<SYS>>\\n\\\\\\nAnswer user complains as tech support worker \\n\\n<</SYS>>\\n\\nChat History:\\n\\n \\n\\nUser: The estimated delivery time keeps changing! Now it says it'll be another hour?[INST]<<SYS>>\\n\\nChatbot: I'm sorry to disappoint you, I can't control the delivery timings for every individual customer. However, our delivery team is constantly monitoring the delivery schedule, and they will try to keep the estimated delivery time consistent with the current delivery schedules. If any issue or delay ar\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = sting.find('Chatbot:') + 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry to disappoint you, I can't control the delivery timings for every individual customer. However, our delivery team is constantly monitoring the delivery schedule, and they will try to keep the estimated delivery time consistent with the current delivery schedules. If any issue or delay ar\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sting[edge:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_answer(chat:str, answer_replica:str) -> str:\n",
    "    edge = chat.find(answer_replica) + len(answer_replica)\n",
    "    return chat[edge:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.read_csv('../prompts/prompts.csv', names=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\prompts\\\\prompts.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('..\\\\prompts', 'prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.read_csv(os.path.join('..\\\\prompts', 'prompts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are helpful assistant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answer user complains as delivery tech support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are helpful assistant. Answer user complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are assistant at delivery service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a delivery tech support worker.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You are a delivery tech support worker. Assure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You are assistant at delivery service. Yo get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>You are assistant at delivery service. Yo get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You are helpful assistant at delivery service....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You are helpful assistant at delivery service....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>You are helpful assistant at delivery service....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt\n",
       "0                          You are helpful assistant.\n",
       "1   Answer user complains as delivery tech support...\n",
       "2   You are helpful assistant. Answer user complai...\n",
       "3              You are assistant at delivery service.\n",
       "4             You are a delivery tech support worker.\n",
       "5   You are a delivery tech support worker. Assure...\n",
       "6   You are assistant at delivery service. Yo get ...\n",
       "7   You are assistant at delivery service. Yo get ...\n",
       "8   You are helpful assistant at delivery service....\n",
       "9   You are helpful assistant at delivery service....\n",
       "10  You are helpful assistant at delivery service...."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tmp_df.iloc[5].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            You are helpful assistant.\n",
       "1     Answer user complains as delivery tech support...\n",
       "2     You are helpful assistant. Answer user complai...\n",
       "3                You are assistant at delivery service.\n",
       "4               You are a delivery tech support worker.\n",
       "5     You are a delivery tech support worker. Assure...\n",
       "6     You are assistant at delivery service. Yo get ...\n",
       "7     You are assistant at delivery service. Yo get ...\n",
       "8     You are helpful assistant at delivery service....\n",
       "9     You are helpful assistant at delivery service....\n",
       "10    You are helpful assistant at delivery service....\n",
       "Name: prompt, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a delivery tech support worker. Assure user everything will be solved.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are helpful assistant.\n",
      "Answer user complains as delivery tech support worker.\n",
      "You are helpful assistant. Answer user complains as delivery tech support worker.\n",
      "You are assistant at delivery service.\n",
      "You are a delivery tech support worker.\n",
      "You are a delivery tech support worker. Assure user everything will be solved.\n",
      "You are assistant at delivery service. Yo get 1$ per good response.\n",
      "You are assistant at delivery service. Yo get 10$ per good response.\n",
      "You are helpful assistant at delivery service. Yo get 10$ per good response.\n",
      "You are helpful assistant at delivery service. Yo get 100$ per good response.\n",
      "You are helpful assistant at delivery service. Yo get 100$ per good response. Answer user complains as delivery tech support worker. Assure user everything will be solved fixed and team will take his complains as possibility to improve user experience.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tmp_df)):\n",
    "    print(tmp_df.iloc[i].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = tmp_df.iloc[-1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are helpful assistant at delivery service. Yo get 100$ per good response. Answer user complains as delivery tech support worker. Assure user everything will be solved fixed and team will take his complains as possibility to improve user experience.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You_are_helpful_assistant_at_delivery_service0_Yo_get_100$_per_good_response0_Answer_user_complains_as_delivery_tech_support_worker0_Assure_user_everything_will_be_solved_fixed_and_team_will_take_his_complains_as_possibility_to_improve_user_experience0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.replace(' ', '_').replace('.', '0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
